{
  "experiment_id": "C03",
  "created_at": "2025-11-17T03:39:27.760120+00:00",
  "dataset_path": "data/Big_AHR.csv",
  "cleaning": "lemmatize",
  "nlp_method": "keras_tokenizer",
  "embedding": "learned",
  "embedding_dim": 128,
  "max_len": 256,
  "vocab_size": 25223,
  "num_folds": 3,
  "folds": [
    1,
    2,
    3
  ],
  "tokenizer_path": "/home/david/github/DeepLearningP2/artifacts/data/C03/tokenizer.json",
  "vectorizer_path": null,
  "embedding_matrix": null,
  "clean_cache": "artifacts/cache/clean_lemmatize.joblib",
  "fold_indices_path": "artifacts/cache/folds_seed42_k3.json",
  "notes": "C03 lemmatize+tokenizer+learned",
  "seed": 42
}